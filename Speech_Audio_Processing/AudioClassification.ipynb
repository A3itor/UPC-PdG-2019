{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AudioClassification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH0Q9Xm12Jgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import torch\n",
        "from torchvision import models, transforms, datasets\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "import pickle\n",
        "import hashlib\n",
        "import librosa\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz65D-Ef-MON",
        "colab_type": "code",
        "outputId": "f9ad3e32-44aa-4774-e518-3ef938efb1e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!pip install gdown\n",
        "!gdown \"https://drive.google.com/uc?id=1HR28jRFwrveq5zxjkzri61jm9F4-M7p7\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.11.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.28.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.6)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1HR28jRFwrveq5zxjkzri61jm9F4-M7p7\n",
            "To: /content/speech_commands_v0.01_with_splits.tar.gz\n",
            "1.49GB [00:09, 152MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXrPdgArVSkm",
        "colab_type": "code",
        "outputId": "907293e9-abe8-48a9-88fd-7c4819d557d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "!gdown \"https://drive.google.com/uc?id=125ghIxEQVIfOvizEU9ZhdO8kjNEta5Kc\"\n",
        "!tar -zxf speech_commands_cached.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=125ghIxEQVIfOvizEU9ZhdO8kjNEta5Kc\n",
            "To: /content/speech_commands_cached.tar.gz\n",
            "864MB [00:15, 56.7MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZjThPIR-cwI",
        "colab_type": "code",
        "outputId": "198ef9d7-626b-4da5-ab35-a386c1ec6793",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "!ls -laorth"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 2.2G\n",
            "drwxr-xr-x  1 root 4.0K Mar 27 20:25 .config\n",
            "drwxr-xr-x  1 root 4.0K Mar 27 20:26 sample_data\n",
            "drwxr-xr-x 32 1000 4.0K Mar 29 11:48 mini\n",
            "drwxr-xr-x  1 root 4.0K Apr  2 17:54 ..\n",
            "-rw-r--r--  1 root  362 Apr  2 17:56 cookie\n",
            "-rw-r--r--  1 root 2.2M Apr  2 17:56 mini_gcommands.tar.gz\n",
            "-rw-r--r--  1 root 1.4G Apr  2 18:08 speech_commands_v0.01_with_splits.tar.gz\n",
            "-rw-r--r--  1 root 825M Apr  2 18:08 speech_commands_cached.tar.gz\n",
            "drwxr-xr-x  1 root 4.0K Apr  2 18:08 .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjlQomTrBrGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -zxf speech_commands_v0.01_with_splits.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdwsRHAH4VJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AudioReader(data.Dataset):\n",
        "  \n",
        "    def __init__(self, list_path, add_noise=False, use_cache=True):\n",
        "        \n",
        "        self.list_path = list_path\n",
        "        self.database_path = os.path.dirname(list_path) + '/audio/'\n",
        "        self.cache_path = os.path.dirname(list_path) + '/cache/'\n",
        "        self.add_noise = add_noise\n",
        "        self.use_cache = use_cache\n",
        "\n",
        "        self.target_class = {}\n",
        "        self.target_class_idx_to_name = {}\n",
        "        self.target_class_names = ['unknown','silence', 'yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
        "        for i, name in enumerate(self.target_class_names):\n",
        "            self.target_class[name] = i\n",
        "            self.target_class_idx_to_name[i] = name\n",
        "        self.audio_class = {}\n",
        "        self.audio_speaker = {}\n",
        "        self.audios = []\n",
        "                 \n",
        "        self.read_list_data()\n",
        "                 \n",
        "        self.speaker_ids = {}\n",
        "        for i, spk_id in enumerate(self.audio_speaker.values()):\n",
        "            self.speaker_ids[spk_id] = i\n",
        "        \n",
        "        \n",
        "        self.num_silence = 0\n",
        "        \n",
        "        if self.add_noise:\n",
        "            self.background_noises_names = []\n",
        "            self.background_noises = []\n",
        "            for f in os.listdir(self.database_path + '/_background_noise_/'):\n",
        "                if f.endswith(\".wav\"):\n",
        "                    self.background_noises_names.append('_background_noise_/' + f)\n",
        "                    print(self.background_noises_names[-1])\n",
        "                    self.background_noises.append( self.load_audio(self.background_noises_names[-1]))\n",
        "\n",
        "            self.num_silence = sum(1 for i in self.audio_class.values() if i == 'yes')\n",
        "            \n",
        "        self.seeded = False\n",
        "        \n",
        "        \n",
        "    \n",
        "    def read_list_data(self):\n",
        "        with open(self.list_path, 'r') as stream:\n",
        "            for line in stream:\n",
        "                file_path = line.strip()\n",
        "                file_class, file_name = file_path.split('/')\n",
        "                identity = file_name.split('_')[0]\n",
        "                self.audio_class[file_path] = file_class\n",
        "                self.audio_speaker[file_path] = identity\n",
        "                self.audios.append(file_path)\n",
        "                \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.audio_class) + self.num_silence\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        if not self.seeded:\n",
        "            self.seeded = True\n",
        "            np.random.seed(index)\n",
        "            \n",
        "        if index >= len(self.audios):\n",
        "            spk_id=-1\n",
        "            length = 22050\n",
        "            audio = self.get_silence_chunk(length)\n",
        "            target_id = self.target_class['silence']\n",
        "            audio_id = 'random_silence/randomchunk.wav' \n",
        "            params = self.getParams(audio)\n",
        "            \n",
        "            #return [params, spk_id, target_id, audio_id]\n",
        "            return [params, target_id]\n",
        "        \n",
        "        audio_id = self.audios[index]\n",
        "        if self.use_cache==False or os.path.isfile(self.cache_path + audio_id + '.pickle') == False:\n",
        "            audio = self.load_audio(audio_id)\n",
        "            params = self.getParams(audio)\n",
        "            if self.use_cache:\n",
        "                dest_path = os.path.dirname(self.cache_path + audio_id + '.pickle')\n",
        "                if not os.path.isdir(dest_path):\n",
        "                    os.makedirs(dest_path)\n",
        "                with open(self.cache_path + audio_id + '.pickle','wb') as stream:\n",
        "                    pickle.dump(params, stream)\n",
        "        else:\n",
        "            with open(self.cache_path + audio_id + '.pickle','rb') as stream:\n",
        "                params = pickle.load(stream)\n",
        "\n",
        "        spk_id = self.speaker_ids[self.audio_speaker[audio_id]]\n",
        "        target = self.audio_class[audio_id]\n",
        "                \n",
        "\n",
        "        if target not in self.target_class_names:\n",
        "            target = 'unknown'\n",
        "        target_id = self.target_class[target]\n",
        "        \n",
        "            \n",
        "        #return [params, spk_id, target_id, audio_id]\n",
        "        return [params, target_id]\n",
        "        \n",
        "        \n",
        "    def load_audio(self, audio_name):\n",
        "        \n",
        "        audio_path = self.database_path + audio_name\n",
        "        #fs, audio = wavfile.read(audio_path)\n",
        "        audio, fs = librosa.load(audio_path)\n",
        "\n",
        "        #audio = audio / 2**15\n",
        "        return audio\n",
        "                 \n",
        "    def getParams(self, y, sfr=22050, window_stride=0.01, window_size=0.02, window_type='hamming', n_fft=512, normalize=True, max_len=97):\n",
        "        #audio_path = self.database_path + audio_name\n",
        "        #y, sfr = librosa.load(audio_path)\n",
        "        win_length = int(sfr * window_size)\n",
        "        hop_length = int(sfr * window_stride)\n",
        "        lowfreq = 20\n",
        "        highfreq = sfr/2 - 400\n",
        "        S = librosa.stft(y, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=window_type, center=False)\n",
        "        D = np.abs(S)\n",
        "        param = librosa.feature.melspectrogram(S=D, sr=sfr, n_mels=40, fmin=lowfreq, fmax=highfreq, norm=None)\n",
        "        #mfcc_default = librosa.feature.mfcc(y=y, sr=sfr, n_mfcc=40)\n",
        "        #mfcc = librosa.feature.mfcc(S=librosa.power_to_db(D), n_mfcc=13)\n",
        "            \n",
        "        # Add zero padding to make all param with the same dims\n",
        "        if param.shape[1] < max_len:\n",
        "            pad = np.zeros((param.shape[0], max_len - param.shape[1]))\n",
        "            param = np.hstack((pad, param))\n",
        "\n",
        "        # If exceeds max_len keep last samples\n",
        "        elif param.shape[1] > max_len:\n",
        "            param = param[:, -max_len:]\n",
        "        param = param.reshape(1,40,97)\n",
        "\n",
        "        param = torch.FloatTensor(param)\n",
        "\n",
        "        # z-score normalization\n",
        "        if normalize:\n",
        "            mean = param.mean()\n",
        "            std = param.std()\n",
        "            if std != 0:\n",
        "                param.add_(-mean)\n",
        "                param.div_(std)\n",
        "\n",
        "        return param    \n",
        "    \n",
        "    \n",
        "    def get_silence_chunk(self, length):\n",
        "        i = np.random.randint(0, len(self.background_noises))\n",
        "        silence = self.background_noises[i]\n",
        "        max_start = silence.shape[0] - length -1\n",
        "        random_start = np.random.randint(0, max_start)\n",
        "        #print(\"Starting at\", random_start )\n",
        "        chunk = silence[random_start:(random_start + length)]\n",
        "        return chunk\n",
        "      \n",
        "    def get_class_weights(self):\n",
        "        class_ids = []\n",
        "        for target in self.audio_class.values():\n",
        "            if target not in self.target_class_names:\n",
        "                target = 'unknown'\n",
        "            target_id = self.target_class[target]\n",
        "            class_ids.append(target_id)\n",
        "        for jj in range(self.num_silence):\n",
        "            class_ids.append(self.target_class['silence'])\n",
        "        class_ids.append(self.target_class['unknown'])\n",
        "        from sklearn.utils import class_weight\n",
        "        #print(np.unique(class_ids))\n",
        "        class_weight = class_weight.compute_class_weight('balanced', np.unique(class_ids),class_ids)\n",
        "        class_weight = torch.from_numpy(class_weight).float()\n",
        "        return class_weight\n",
        "      \n",
        "    def get_n_classes(self):\n",
        "        return len(self.target_class_names)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myp0dMin9tIV",
        "colab_type": "code",
        "outputId": "0d3dcfee-f89c-44a1-fbfe-91836f2ead35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "train_loader = data.DataLoader(\n",
        "                    AudioReader('gcommands/training_list.txt',add_noise=True), \n",
        "                        batch_size=50, shuffle=True, num_workers=2, pin_memory=True, \n",
        "                    )\n",
        "\n",
        "valid_loader = data.DataLoader(\n",
        "                    AudioReader('gcommands/validation_list.txt'), \n",
        "                        batch_size=50, shuffle=False, num_workers=2, pin_memory=True, \n",
        "                    )\n",
        "\n",
        "test_loader = data.DataLoader(\n",
        "                    AudioReader('gcommands/testing_list.txt'), \n",
        "                        batch_size=50, shuffle=False, num_workers=2, pin_memory=True, \n",
        "                    )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_background_noise_/pink_noise.wav\n",
            "_background_noise_/white_noise.wav\n",
            "_background_noise_/running_tap.wav\n",
            "_background_noise_/dude_miaowing.wav\n",
            "_background_noise_/exercise_bike.wav\n",
            "_background_noise_/doing_the_dishes.wav\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g3Prx-aHO6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for batch in train_loader:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-O7ZJPqJWB6",
        "colab_type": "code",
        "outputId": "b8ee0fde-f3ff-44f3-fd2a-a6bfe5b5ed15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X, target = batch\n",
        "print(X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([50, 1, 40, 97])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5FtsCWQP2T7",
        "colab_type": "code",
        "outputId": "b70b65a1-c0d7-4ebf-e9f9-869f02fc2f4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "train_loader.dataset.get_class_weights()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1356, 2.3723, 2.3723, 2.3812, 2.3941, 2.3954, 2.3994, 2.3825, 2.3672,\n",
              "        2.3994, 2.3408, 2.3710])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WREbkaZkMi0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self, num_classes=31):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(20, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(2940, 1000)\n",
        "        self.fc2 = nn.Linear(1000, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IJLm4mCGkIY",
        "colab_type": "code",
        "outputId": "7b291832-5e37-4706-cb9b-faef572a5c95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#example https://github.com/GRAAL-Research/poutyne/blob/master/examples/mnist.ipynb\n",
        "!pip install poutyne"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: poutyne in /usr/local/lib/python3.6/dist-packages (0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from poutyne) (1.14.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from poutyne) (1.0.1.post2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xscpksOGcM2D",
        "colab_type": "code",
        "outputId": "aabf6f3a-c066-40f9-c9a0-5d90c07cad6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pip install torchsummary"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-tVvdboG4Vj",
        "colab_type": "code",
        "outputId": "c8f8e17c-9dfa-453a-9f68-57d91c74e1c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        }
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from poutyne.framework import Model\n",
        "from torchsummary import summary\n",
        "cuda_device = 0\n",
        "device = torch.device(\"cuda:%d\" % cuda_device if torch.cuda.is_available() else \"cpu\")\n",
        "                      \n",
        "mymodel = LeNet(num_classes = train_loader.dataset.get_n_classes())\n",
        "#mymodel = TDNN(num_classes = train_loader.dataset.get_n_classes())\n",
        "#mymodel = VGG('VGG11',num_classes = train_loader.dataset.get_n_classes())\n",
        "#mymodel = MyVGG(num_classes = train_loader.dataset.get_n_classes())\n",
        "#mymodel = MyVGGUpsample(num_classes = train_loader.dataset.get_n_classes())\n",
        "print(mymodel.to(device))\n",
        "summary(mymodel, input_size=(1, 40, 97))\n",
        "learning_rate = 0.001\n",
        "# Optimizer and loss function\n",
        "#optimizer = optim.SGD(mymodel.parameters(), lr=learning_rate, weight_decay=0.001)\n",
        "#optimizer = optim.Adam(mymodel.parameters(), lr=learning_rate)\n",
        "optimizer = optim.Adam( filter(lambda p: p.requires_grad, mymodel.parameters()), lr=learning_rate )\n",
        "loss_function = nn.CrossEntropyLoss(weight=train_loader.dataset.get_class_weights())\n",
        "\n",
        "\n",
        "model = Model(mymodel, optimizer, loss_function, metrics=['accuracy'])\n",
        "\n",
        "# Send model on GPU\n",
        "model.to(device)\n",
        "\n",
        "model.fit_generator(train_loader, valid_loader, epochs=1)\n",
        "\n",
        "for param in mymodel.parameters():\n",
        "    param.requires_grad = True\n",
        "    \n",
        "optimizer = optim.Adam( filter(lambda p: p.requires_grad, mymodel.parameters()), lr=learning_rate )\n",
        "loss_function = nn.CrossEntropyLoss(weight=train_loader.dataset.get_class_weights())\n",
        "\n",
        "\n",
        "model = Model(mymodel, optimizer, loss_function, metrics=['accuracy'])\n",
        "\n",
        "# Send model on GPU\n",
        "model.to(device)\n",
        "\n",
        "model.fit_generator(train_loader, valid_loader, epochs=10)\n",
        "\n",
        "\n",
        " # Test\n",
        "test_loss, test_acc = model.evaluate_generator(test_loader)\n",
        "print('Test:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format(test_loss, test_acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LeNet(\n",
            "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(20, 20, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2_drop): Dropout2d(p=0.5)\n",
            "  (fc1): Linear(in_features=2940, out_features=1000, bias=True)\n",
            "  (fc2): Linear(in_features=1000, out_features=12, bias=True)\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 20, 36, 93]             520\n",
            "            Conv2d-2           [-1, 20, 14, 42]          10,020\n",
            "         Dropout2d-3           [-1, 20, 14, 42]               0\n",
            "            Linear-4                 [-1, 1000]       2,941,000\n",
            "            Linear-5                   [-1, 12]          12,012\n",
            "================================================================\n",
            "Total params: 2,963,552\n",
            "Trainable params: 2,963,552\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.70\n",
            "Params size (MB): 11.31\n",
            "Estimated Total Size (MB): 12.02\n",
            "----------------------------------------------------------------\n",
            "Epoch 1/1 36.23s Step 1059/1059: loss: 1.276418, acc: 45.801542, val_loss: 0.816586, val_acc: 71.977052\n",
            "Epoch 1/10 35.38s Step 1059/1059: loss: 0.721934, acc: 64.059077, val_loss: 0.722055, val_acc: 74.742571\n",
            "Epoch 2/10 35.37s Step 1059/1059: loss: 0.606631, acc: 68.929894, val_loss: 0.566873, val_acc: 79.964696\n",
            "Epoch 3/10 35.27s Step 1059/1059: loss: 0.524770, acc: 72.329455, val_loss: 0.588027, val_acc: 79.567520\n",
            "Epoch 4/10 35.39s Step 1059/1059: loss: 0.482409, acc: 73.608068, val_loss: 0.653797, val_acc: 76.831421\n",
            "Epoch 5/10 35.38s Step 1059/1059: loss: 0.447703, acc: 75.390950, val_loss: 0.592447, val_acc: 79.699912\n",
            "Epoch 6/10 34.95s Step 1059/1059: loss: 0.409846, acc: 77.153056, val_loss: 0.515931, val_acc: 82.538982\n",
            "Epoch 7/10 35.59s Step 1059/1059: loss: 0.389483, acc: 77.951953, val_loss: 0.481446, val_acc: 83.612827\n",
            "Epoch 8/10 35.23s Step 1059/1059: loss: 0.377537, acc: 78.346680, val_loss: 0.523226, val_acc: 82.244778\n",
            "Epoch 9/10 35.39s Step 1059/1059: loss: 0.362088, acc: 79.138022, val_loss: 0.449460, val_acc: 84.980877\n",
            "Epoch 10/10 35.41s Step 1059/1059: loss: 0.356219, acc: 79.568633, val_loss: 0.515087, val_acc: 82.833186\n",
            "Test:\n",
            "\tLoss: 0.48600401588727193\n",
            "\tAccuracy: 83.02852974412383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZWf9GbTwXvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # Test\n",
        "test_loss, test_acc = model.evaluate_generator(test_loader)\n",
        "print('Test:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format(test_loss, test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnZmZRqkmgno",
        "colab_type": "code",
        "outputId": "ee0206b0-cc54-4507-93b0-c2cd8aa72a0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        }
      },
      "source": [
        "for p in filter(lambda p: p.requires_grad, mymodel.parameters()):\n",
        "  print(p)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0101,  0.0063,  0.0089,  ...,  0.0052, -0.0070,  0.0103],\n",
            "        [-0.0115, -0.0152, -0.0133,  ..., -0.0118, -0.0284, -0.0201],\n",
            "        [-0.0128,  0.0035,  0.0087,  ...,  0.0211, -0.0018, -0.0268],\n",
            "        ...,\n",
            "        [ 0.0137,  0.0242, -0.0078,  ...,  0.0081, -0.0040,  0.0002],\n",
            "        [-0.0064, -0.0052,  0.0116,  ...,  0.0010,  0.0238,  0.0113],\n",
            "        [ 0.0118,  0.0138, -0.0226,  ...,  0.0075, -0.0243, -0.0253]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0181, -0.0250, -0.0204,  ..., -0.0225, -0.0211, -0.0100],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0046,  0.0052, -0.0101,  ..., -0.0130, -0.0116, -0.0010],\n",
            "        [-0.0029,  0.0023, -0.0067,  ...,  0.0043,  0.0054, -0.0095],\n",
            "        [ 0.0107,  0.0035,  0.0114,  ..., -0.0126,  0.0160,  0.0016],\n",
            "        ...,\n",
            "        [ 0.0041,  0.0075, -0.0055,  ..., -0.0203,  0.0002, -0.0007],\n",
            "        [ 0.0043, -0.0104,  0.0129,  ...,  0.0165,  0.0063, -0.0096],\n",
            "        [-0.0055,  0.0106,  0.0186,  ..., -0.0112, -0.0075,  0.0110]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0079,  0.0024, -0.0128,  ...,  0.0051,  0.0016,  0.0126],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0082,  0.0057,  0.0095,  ...,  0.0133, -0.0083, -0.0164],\n",
            "        [-0.0072, -0.0096, -0.0026,  ..., -0.0076,  0.0108,  0.0124],\n",
            "        [-0.0033, -0.0088, -0.0085,  ..., -0.0049, -0.0159,  0.0007],\n",
            "        ...,\n",
            "        [-0.0034, -0.0122,  0.0089,  ...,  0.0091, -0.0062,  0.0142],\n",
            "        [-0.0065, -0.0063,  0.0043,  ..., -0.0141,  0.0048, -0.0090],\n",
            "        [ 0.0138, -0.0085,  0.0067,  ..., -0.0140,  0.0160,  0.0167]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0088, -0.0055, -0.0106, -0.0044, -0.0136,  0.0043, -0.0070, -0.0136,\n",
            "        -0.0078,  0.0125,  0.0114, -0.0059], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.1198,  0.2053, -0.0373],\n",
            "          [-0.1828,  0.1416,  0.0248],\n",
            "          [-0.1437,  0.2047, -0.1715]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2131,  0.0547, -0.1319],\n",
            "          [ 0.2616, -0.1630, -0.3050],\n",
            "          [ 0.0421, -0.0357, -0.0319]]],\n",
            "\n",
            "\n",
            "        [[[-0.0284,  0.3276, -0.0621],\n",
            "          [-0.2214, -0.0652, -0.2388],\n",
            "          [-0.0930, -0.2041,  0.3051]]]], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.2905, -0.0724, -0.0751], device='cuda:0', requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kELJsdfVjpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TDNN(nn.Module):\n",
        "    def __init__(self, num_classes=12):\n",
        "        super(TDNN, self).__init__()\n",
        "        self.tdnn = nn.Sequential(\n",
        "            nn.Conv1d(40, 450, stride=1, dilation=1, kernel_size=3),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(450, 450, stride=1, dilation=1, kernel_size=4),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(450, 450, stride=1, dilation=3, kernel_size=3),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(450, 450, stride=1, dilation=3, kernel_size=3),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(450, 450, stride=1, dilation=3, kernel_size=3),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(450, 450, stride=1, dilation=3, kernel_size=3),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(450, 450, stride=1, dilation=3, kernel_size=3),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool1d(3, stride=3),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(9000, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x.squeeze_(1)\n",
        "        x = self.tdnn(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDkfbFbweeZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n",
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(self, vgg_name, num_classes=12):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1 * 3 * 512, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        print(x.shape)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        print(x.shape)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "def make_layers(cfg, batch_norm=True):\n",
        "    layers = []\n",
        "    in_channels = 1\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEsZgiTzgTsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MyVGG(nn.Module):\n",
        "    def __init__(self, num_classes=12):\n",
        "          super(MyVGG, self).__init__()\n",
        "          self.origVGG = models.vgg11()\n",
        "          for param in self.origVGG.parameters():\n",
        "              param.requires_grad = False\n",
        "          self.origVGG.classifier = nn.Sequential(\n",
        "              nn.Linear(1 * 3 * 512, 4096),\n",
        "              nn.ReLU(True),\n",
        "              nn.Dropout(),\n",
        "              nn.Linear(4096, 4096),\n",
        "              nn.ReLU(True),\n",
        "              nn.Dropout(),\n",
        "              nn.Linear(4096, num_classes),\n",
        "          )\n",
        "          self.conv1 = nn.Conv2d(1, 3, kernel_size=3, padding = 1)\n",
        "          \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        #print(x.shape)\n",
        "        x = self.origVGG.features(x)\n",
        "        #print(x.shape)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        #print(x.shape)\n",
        "        x = self.origVGG.classifier(x)\n",
        "        #print(x.shape)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBZy5VE1jDvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MyVGGUpsample(nn.Module):\n",
        "    def __init__(self, num_classes=12):\n",
        "          super(MyVGGUpsample, self).__init__()\n",
        "          self.origVGG = models.vgg11()\n",
        "          for param in self.origVGG.parameters():\n",
        "              param.requires_grad = False\n",
        "          \n",
        "          removed = list(self.origVGG.classifier.children())[:-4]\n",
        "          composed = removed + [nn.Linear(4096, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, num_classes)]\n",
        "          self.origVGG.classifier = nn.Sequential(*composed)\n",
        "          \n",
        "          #self.conv1 = nn.Conv2d(1, 3, kernel_size=3, padding = 1)\n",
        "          self.upsample1 = nn.ConvTranspose2d(in_channels = 1, out_channels = 1, kernel_size = (4,1), stride = (2,1), padding = 0)\n",
        "          self.upsample2 = nn.ConvTranspose2d(in_channels = 1, out_channels = 3, kernel_size = (6,4), stride = (3,2), padding = 0)\n",
        "          \n",
        "    def forward(self, x):\n",
        "        x = self.upsample2(self.upsample1(x))\n",
        "        x = self.origVGG.forward(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}